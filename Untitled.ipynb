{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baogong/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "#conf = SparkConf().setAppName(\"baogong toturial\") \n",
    "conf = SparkConf().setAppName(\"baogong toturial\") \\\n",
    "    .set(\"spark.driver.maxResultSize\", \"16g\") \\\n",
    "    .set('spark.driver.memory','32g') \\\n",
    "    .set(\"spark.sql.parquet.binaryAsString\", \"true\") \\\n",
    "    .set(\"spark.files.overwrite\", \"true\") \\\n",
    "    .set(\"spark.akka.frameSize\", \"60\") \\\n",
    "    .set(\"spark.hadoop.validateOutputSpecs\", \"false\") \\\n",
    "    .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .set(\"spark.executor.extraJavaOptions\", \"-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\") \\\n",
    "    .set(\"spark.storage.memoryFraction\", \"0.8\") \\\n",
    "    .set(\"spark.kryoserializer.buffer.max\", \"2000m\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = HiveContext(sc)\n",
    "#sqlContext.setConf(\"fs.defaultFS\", \"hdfs://mgjcluster\")\n",
    "sqlContext.sql(\"set spark.sql.hive.convertMetastoreOrc=true\")\n",
    "sqlContext.sql(\"set hive.exec.orc.split.strategy=ETL\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_white_data(date1, date2):\n",
    "    sql = \"\"\"\n",
    "        select *\n",
    "        from baogong_uuid_white_simulator_url\n",
    "        --tablesample(bucket 1 out of 8 on rand())\n",
    "        where visit_date>='{}'\n",
    "        and visit_date<='{}'\n",
    "        --limit 500000\n",
    "    \"\"\".format(date1, date2)\n",
    "    data = sqlContext.sql(sql).toPandas()\n",
    "    return data\n",
    "\n",
    "def get_black_data(date1, date2):\n",
    "    sql = \"\"\"\n",
    "        select *\n",
    "        from baogong_uuid_black_simulator_url\n",
    "        where visit_date>='{}'\n",
    "        and visit_date<='{}'\n",
    "    \"\"\".format(date1, date2)\n",
    "    data = sqlContext.sql(sql).toPandas()\n",
    "    return data\n",
    "\n",
    "def get_data(date1, date2):\n",
    "    sql = \"\"\"\n",
    "        select uuid_did, words, times\n",
    "        from baogong_uuid_sequence_deeplearning_page_data_app\n",
    "        where visit_date>='{}'\n",
    "        and visit_date<='{}'\n",
    "    \"\"\".format(date1, date2)\n",
    "    data = sqlContext.sql(sql).toPandas()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dic = np.load('./words_dic.npy').item()\n",
    "words_dic['UNK'] = len(words_dic)+1\n",
    "words_dic['UNKOWN'] = 0 #masking\n",
    "\n",
    "white_data = get_white_data('2019-09-25', '2019-09-25')\n",
    "black_data = get_black_data('2019-06-01', '2019-09-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baogong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "white_data['label'] = 0\n",
    "black_data['label'] = 1\n",
    "data = pd.concat([white_data, black_data], axis = 0, ignore_index=True)\n",
    "\n",
    "data['len'] = data['words'].apply(lambda x: len(x.split('::')))\n",
    "\n",
    "data1 = data[data['len']>=5]\n",
    "data1['label'].value_counts()\n",
    "\n",
    "data_deep = data1[['words', 'times', 'label']]\n",
    "data_deep['times'] = data_deep['times'].fillna('0')\n",
    "data_deep = data_deep.fillna('UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff_parse(times):\n",
    "    time = list(map(lambda x: int(x), times))\n",
    "    head_time = time[:-1]\n",
    "    tail_time = time[1:]\n",
    "    time_diff = []\n",
    "    for head, tail in zip(head_time, tail_time):\n",
    "        time_diff.append(head - tail)\n",
    "    time_diff.append(999)\n",
    "    time_diff_result = []\n",
    "    for diff in time_diff:\n",
    "        if diff <= 0:\n",
    "            time_diff_result.append(1)  #与padding 0做区分\n",
    "        elif diff < 2:\n",
    "            time_diff_result.append(2)\n",
    "        elif diff < 4:\n",
    "            time_diff_result.append(3)\n",
    "        elif diff < 8:\n",
    "            time_diff_result.append(4)\n",
    "        elif diff < 16:\n",
    "            time_diff_result.append(5)\n",
    "        elif diff < 32:\n",
    "            time_diff_result.append(6)\n",
    "        elif diff < 64:\n",
    "            time_diff_result.append(7)\n",
    "        elif diff < 128:\n",
    "            time_diff_result.append(8)\n",
    "        elif diff < 1000:\n",
    "            time_diff_result.append(9)\n",
    "        else:\n",
    "            time_diff_result.append(10)\n",
    "    return time_diff_result\n",
    "\n",
    "def get_convert_data(Data, max_len, low_len, words_dic):\n",
    "    X_url = []\n",
    "    X_time = []\n",
    "    length = []\n",
    "    for i in tqdm(range(Data.shape[0])):\n",
    "        lines = Data.iloc[i, :]\n",
    "        urls, times = lines[0].split('::'), lines[1].split('::')\n",
    "\n",
    "        times = time_diff_parse(times)\n",
    "        \n",
    "        l = len(urls)\n",
    "        #  对数据进行 补长 去短\n",
    "        #assert len(urls) == len(times)\n",
    "        if len(urls) > max_len:\n",
    "            urls = urls[:max_len]\n",
    "            times = times[:max_len]\n",
    "        else:\n",
    "            urls = urls + (max_len - len(urls)) * ['UNKOWN']  #UNKOWN->0做mask，UNK->len做未知\n",
    "            times = times + (max_len - len(times)) * [0]  \n",
    "        url_id = list(map(lambda x: words_dic[x] if x in words_dic else words_dic['UNK'], urls))\n",
    "        time_id = times#time_diff_parse(times)\n",
    "        url_id = url_id if len(url_id)==max_len else url_id + [words_dic['UNK']]*(max_len-len(url_id))\n",
    "        time_id = time_id if len(time_id)==max_len else time_id + [0]*(max_len-len(time_id))\n",
    "        assert len(url_id) == len(time_id) == max_len\n",
    "\n",
    "        X_url.append(url_id)\n",
    "        X_time.append(time_id)\n",
    "        length.append(l)\n",
    "    return X_url, X_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1714635/1714635 [11:43<00:00, 2436.88it/s]\n"
     ]
    }
   ],
   "source": [
    "X_url, X_time = get_convert_data(data_deep, 200, 1, words_dic)\n",
    "\n",
    "url_train, url_test = train_test_split(np.array(X_url), random_state=11)\n",
    "time_train, time_test = train_test_split(np.array(X_time), random_state=11)\n",
    "y_train, y_test = train_test_split(data_deep['label'].values, random_state=11)\n",
    "\n",
    "x_train_set_all = {\"words\": url_train,\n",
    "                   \"times\": time_train,\n",
    "                   }\n",
    "x_test_set_all = {\"words\": url_test,\n",
    "                  \"times\": time_test,\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layerNormalization(Layer):\n",
    "    def __init__(self, l2_rate, epsilon = 1e-8, **kwargs):\n",
    "        self.l2_rate = l2_rate\n",
    "        self.epsilon = epsilon\n",
    "        super(layerNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.w = self.add_weight(name='kernel',\n",
    "                                        shape=(input_dim,),\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        regularizer=l2(self.l2_rate),\n",
    "                                        trainable=True)\n",
    "        self.b = self.add_weight(name='bias',\n",
    "                                     shape=(input_dim,),\n",
    "                                     initializer='Zeros',\n",
    "                                     trainable=True)\n",
    "\n",
    "        super(layerNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        x0 = inputs\n",
    "        mean, variance = tf.nn.moments(inputs, axes=[-1], keepdims=True)\n",
    "        normalized = (inputs - mean) / ( (variance + self.epsilon) ** (.5) )\n",
    "        outputs = self.w * normalized + self.b\n",
    "        return outputs\n",
    "    \n",
    "class positionalEncoding(Layer):\n",
    "    \"\"\"\n",
    "    inputs: 3d tensor. (N, T, E)\n",
    "    maxlen: scalar. Must be >= T\n",
    "    masking: Boolean. If True, padding positions are set to zeros.\n",
    "    returns\n",
    "    3d tensor that has the same shape as inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen, masking=True, **kwargs):\n",
    "        self.maxlen = maxlen\n",
    "        self.masking = masking\n",
    "        super(positionalEncoding, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(positionalEncoding, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        E = inputs.get_shape().as_list()[-1]\n",
    "        N, T = tf.shape(inputs)[0], tf.shape(inputs)[1] # dynamic\n",
    "        # position indices\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # (N, T)\n",
    "\n",
    "        # First part of the PE function: sin and cos argument\n",
    "        position_enc = np.array([\n",
    "            [pos / np.power(10000, (i-i%2)/E) for i in range(E)]\n",
    "            for pos in range(self.maxlen)])\n",
    "\n",
    "        # Second part, apply the cosine to even columns and sin to odds.\n",
    "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "        position_enc = tf.convert_to_tensor(position_enc, tf.float32) # (maxlen, E)\n",
    "\n",
    "        # lookup\n",
    "        outputs = tf.nn.embedding_lookup(position_enc, position_ind)\n",
    "\n",
    "        # masks\n",
    "        if self.masking:\n",
    "            outputs = tf.where(tf.equal(inputs, 0), inputs, outputs)\n",
    "        return tf.dtypes.cast(outputs, tf.float32)\n",
    "    \n",
    "#masks = tf.math.equal(x, 0)\n",
    "def mask(inputs, key_masks=None, type=None):\n",
    "    \"\"\"\n",
    "    https://github.com/Kyubyong/transformer/blob/master/modules.py\n",
    "    \"\"\"\n",
    "    padding_num = -2 ** 32 + 1\n",
    "    if type in (\"k\", \"key\", \"keys\"):\n",
    "        key_masks = tf.dtypes.cast(key_masks, tf.float32)\n",
    "        key_masks = tf.tile(key_masks, [tf.shape(inputs)[0] // tf.shape(key_masks)[0], 1]) # (h*N, seqlen)\n",
    "        key_masks = tf.expand_dims(key_masks, 1)  # (h*N, 1, seqlen)\n",
    "        outputs = inputs + key_masks * padding_num\n",
    "    elif type in (\"f\", \"future\", \"right\"):\n",
    "        diag_vals = tf.ones_like(inputs[0, :, :])  # (T_q, T_k)\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n",
    "        future_masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])  # (N, T_q, T_k)\n",
    "\n",
    "        paddings = tf.ones_like(future_masks) * padding_num\n",
    "        outputs = tf.where(tf.equal(future_masks, 0), paddings, inputs)#True的都填充了paddings，false填充了inputs\n",
    "    else:\n",
    "        print(\"Check if you entered type correctly!\")\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, masks, key_masks,\n",
    "                                 causality=False, dropout_rate=0.,\n",
    "                                 training=True):\n",
    "    \"\"\"\"\n",
    "    Q: Packed queries. 3d tensor. [N, T_q, d_k].\n",
    "    K: Packed keys. 3d tensor. [N, T_k, d_k].\n",
    "    V: Packed values. 3d tensor. [N, T_k, d_v]\n",
    "    \"\"\"\n",
    "    dk = Q.get_shape().as_list()[-1]\n",
    "    outputs = tf.matmul(Q, tf.transpose(K, (0, 2, 1)))/dk**0.5\n",
    "    if masks:\n",
    "        outputs = mask(outputs, key_masks=key_masks, type=\"key\")\n",
    "    # causality or future blinding masking\n",
    "    if causality:\n",
    "        outputs = mask(outputs, type=\"future\")\n",
    "    \n",
    "    # softmax\n",
    "    outputs = tf.nn.softmax(outputs) \n",
    "    # dropout\n",
    "    outputs = keras.layers.Dropout(rate=dropout_rate)(outputs)\n",
    "\n",
    "    # weighted sum (context vectors)\n",
    "    outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n",
    "    return outputs\n",
    "\n",
    "class mutiHeadAttention(Layer):\n",
    "    def __init__(self, masks, causality, dropout_rate, training, num_heads, size_per_head, **kwargs):\n",
    "        self.masks = masks\n",
    "        #self.key_masks = key_masks\n",
    "        self.causality = causality\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = training\n",
    "        self.num_heads = num_heads\n",
    "        self.size_per_head = size_per_head\n",
    " \n",
    "        super(mutiHeadAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_shape = self.num_heads * self.size_per_head\n",
    "        \n",
    "        self.q_dense = keras.layers.Dense(input_shape, use_bias=True)\n",
    "        self.k_dense = keras.layers.Dense(input_shape, use_bias=True)\n",
    "        self.v_dense = keras.layers.Dense(input_shape, use_bias=True)\n",
    "        \n",
    "        super(mutiHeadAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        q, k, v , input1 = inputs\n",
    "        \n",
    "        if self.masks:\n",
    "            key_masks = tf.equal(input1, 0)\n",
    "        else:\n",
    "            key_masks = None\n",
    "        Q = self.q_dense(q)\n",
    "        K = self.k_dense(k)\n",
    "        V = self.v_dense(v)\n",
    "        \n",
    "        Q_ = tf.concat(tf.split(Q, self.num_heads, axis=2), axis=0) # (num_heads*N, T_q, size_per_head)\n",
    "        K_ = tf.concat(tf.split(K, self.num_heads, axis=2), axis=0) # (num_heads*N, T_k, size_per_head)\n",
    "        V_ = tf.concat(tf.split(V, self.num_heads, axis=2), axis=0) # (num_heads*N, T_k, size_per_head)\n",
    "        \n",
    "        # Attention\n",
    "        outputs = scaled_dot_product_attention(Q_, K_, V_, self.masks, key_masks, \n",
    "                                               self.causality, self.dropout_rate, self.training)\n",
    "\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, self.num_heads, axis=0), axis=2 ) # (N, T_q, d_model)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += q\n",
    "        return outputs\n",
    "\n",
    "class hanAttention(Layer):\n",
    "    \"\"\"https://arxiv.org/pdf/1707.00896.pdf\"\"\"\n",
    "    def __init__(self, masks, dropout_rate, l2_rate, **kwargs):\n",
    "        self.masks = masks\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_rate = l2_rate\n",
    "        super(hanAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #print ('my shape is,',input_shape[0])\n",
    "        self.w = self.add_weight(name='kernel',\n",
    "                                        shape=(int(input_shape[0][-1]),),\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        regularizer=l2(self.l2_rate),\n",
    "                                        trainable=True\n",
    "                                )\n",
    "        \n",
    "        self.dense = keras.layers.Dense(int(input_shape[0][-1]), use_bias=True)\n",
    "        \n",
    "        super(hanAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        h, input1 = inputs #h [none, 200, 2*lstmShape]\n",
    "        dk = h.get_shape().as_list()[-1]\n",
    "        \n",
    "        if self.masks:\n",
    "            key_masks = tf.equal(input1, 0)\n",
    "        else:\n",
    "            key_masks = None\n",
    "        hw = self.dense(h)\n",
    "        outputs = tf.matmul(hw, tf.reshape(self.w, (dk,-1)))  #[none, 200, 2*lstmShape]*[2*lstmShape, 1]\n",
    "        outputs = mask(tf.transpose(outputs, (0, 2, 1)), key_masks=key_masks, type=\"key\")\n",
    "        outputs = tf.nn.softmax(outputs)\n",
    "        \n",
    "        outputs = keras.layers.Dropout(rate=self.dropout_rate)(outputs)\n",
    "        outputs = tf.matmul(outputs, h) #[none, 1, 200] * [none, 200, 2*lstmShape] \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionModel():\n",
    "    \n",
    "    inputs1 = keras.layers.Input(shape = (200, ), name = 'words')\n",
    "    embd1 = keras.layers.Embedding(output_dim=64, input_dim=len(words_dic)+1, input_length=200)(inputs1)\n",
    "    inputs2 = keras.layers.Input(shape = (200, ), name = 'times')\n",
    "    embd2 = keras.layers.Embedding(output_dim=64, input_dim=10+1, input_length=200)(inputs2)\n",
    "    \n",
    "    #embd1 += positionalEncoding(200, True)(embd1)\n",
    "    att_words = mutiHeadAttention(masks = True, causality = False, dropout_rate = 0.1, \n",
    "        training = True, num_heads = 8, size_per_head = 8)([embd1, embd1, embd1, inputs1])\n",
    "    #att_words = layerNormalization(0.001, epsilon = 1e-8)(att_words)\n",
    "    \n",
    "    att_times = mutiHeadAttention(masks = True, causality = False, dropout_rate = 0.1, \n",
    "        training = True, num_heads = 8, size_per_head = 8)([embd2, embd2, embd2, inputs2])\n",
    "    #att_times = layerNormalization(0.001, epsilon = 1e-8)(att_times)\n",
    "    \n",
    "    att = keras.layers.concatenate(inputs=[att_words, att_times], axis=1)\n",
    "    att = keras.layers.Flatten()(att)\n",
    "    dense = keras.layers.Dense(16, activation='relu')(att)\n",
    "    \n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model_att = keras.Model(inputs=[inputs1, inputs2], outputs=[output])\n",
    "    model_att.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                            metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    #print (model_att.summary())\n",
    "    return model_att\n",
    "\n",
    "def lstmModel():\n",
    "\n",
    "    inputs1 = keras.layers.Input(shape = (200, ), name = 'words')\n",
    "    embd1 = keras.layers.Embedding(output_dim=64, input_dim=len(words_dic)+1, input_length=200)(inputs1)\n",
    "    inputs2 = keras.layers.Input(shape = (200, ), name = 'times')\n",
    "    embd2 = keras.layers.Embedding(output_dim=64, input_dim=10+1, input_length=200)(inputs2)\n",
    "    \n",
    "    lstm_words = keras.layers.LSTM(64)(embd1)\n",
    "    lstm_times = keras.layers.LSTM(64)(embd2)\n",
    "    \n",
    "    lstm = keras.layers.concatenate(inputs=[lstm_words, lstm_times], axis=1)\n",
    "    lstm = keras.layers.Flatten()(lstm)\n",
    "    dense = keras.layers.Dense(16, activation='relu')(lstm)\n",
    "    \n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model_lstm = keras.Model(inputs=[inputs1, inputs2], outputs=[output])\n",
    "    model_lstm.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                            metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    #print (model_lstm.summary())\n",
    "    return model_lstm\n",
    "\n",
    "def RCnnModel():\n",
    "\n",
    "    inputs1 = keras.layers.Input(shape = (200, ), name = 'words')\n",
    "    embd1 = keras.layers.Embedding(output_dim=64, input_dim=len(words_dic)+1, input_length=200)(inputs1)\n",
    "    inputs2 = keras.layers.Input(shape = (200, ), name = 'times')\n",
    "    embd2 = keras.layers.Embedding(output_dim=64, input_dim=10+1, input_length=200)(inputs2)\n",
    "    \n",
    "    steps_hState1, steps_reverse_hState1 = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(64, return_sequences=True, dropout = 0.3), merge_mode = None)(embd1)\n",
    "    steps_hState2, steps_reverse_hState2 = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(64, return_sequences=True, dropout = 0.3), merge_mode = None)(embd2)\n",
    "    \n",
    "    shape = [tf.shape(steps_hState1)[0], 1, tf.shape(steps_hState1)[2]]\n",
    "    c_left1 = tf.concat([tf.zeros(shape), steps_hState1[:, :-1]], axis=1)\n",
    "    c_right1 = tf.concat([steps_reverse_hState1[:, 1:], tf.zeros(shape)], axis=1)\n",
    "    \n",
    "    c_left2 = tf.concat([tf.zeros(shape), steps_hState2[:, :-1]], axis=1)\n",
    "    c_right2 = tf.concat([steps_reverse_hState2[:, 1:], tf.zeros(shape)], axis=1)\n",
    "    \n",
    "    lstm_words = keras.layers.concatenate(inputs=[c_left1, embd1, c_right1], axis=2)\n",
    "    lstm_times = keras.layers.concatenate(inputs=[c_left2, embd2, c_right2], axis=2)\n",
    "    \n",
    "    dense_words = keras.layers.Dense(64, activation='relu')(lstm_words)\n",
    "    dense_times = keras.layers.Dense(64, activation='relu')(lstm_times)\n",
    "    \n",
    "    dense = keras.layers.concatenate([tf.reduce_max(dense_words, axis = 1), \n",
    "                                      tf.reduce_max(dense_times, axis = 1)], axis = 1)\n",
    "    dropout = keras.layers.Dropout(0.3)(dense)\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "    \n",
    "    model_rcnn = keras.Model(inputs=[inputs1, inputs2], outputs=[output])\n",
    "    model_rcnn.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                            metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    #print (model_lstm.summary())\n",
    "    return model_rcnn\n",
    "\n",
    "def HAttention():\n",
    "    inputs1 = keras.layers.Input(shape = (200, ), name = 'words')\n",
    "    embd1 = keras.layers.Embedding(output_dim=64, input_dim=len(words_dic)+1, input_length=200)(inputs1)\n",
    "    inputs2 = keras.layers.Input(shape = (200, ), name = 'times')\n",
    "    embd2 = keras.layers.Embedding(output_dim=64, input_dim=10+1, input_length=200)(inputs2)\n",
    "    \n",
    "    steps_hState1, steps_reverse_hState1 = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True), merge_mode = None)(embd1)\n",
    "    steps_hState2, steps_reverse_hState2 = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True), merge_mode = None)(embd2)\n",
    "    \n",
    "    lstm_words = keras.layers.concatenate(inputs=[steps_hState1, steps_reverse_hState1], axis=2)\n",
    "    lstm_times = keras.layers.concatenate(inputs=[steps_hState2, steps_reverse_hState2], axis=2)\n",
    "    \n",
    "    att_words = hanAttention(masks = True, dropout_rate = 0.1, l2_rate = 0.001)([lstm_words, inputs1])\n",
    "    att_times = hanAttention(masks = True, dropout_rate = 0.1, l2_rate = 0.001)([lstm_times, inputs1])\n",
    "    \n",
    "    dense = keras.layers.concatenate([tf.reduce_max(att_words, axis = 1), \n",
    "                                      tf.reduce_max(att_times, axis = 1)], axis = 1)\n",
    "    dense = keras.layers.Dense(32, activation='relu')(dense)\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model_han = keras.Model(inputs=[inputs1, inputs2], outputs=[output])\n",
    "    model_han.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                            metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    #print (model_lstm.summary())\n",
    "    return model_han\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1021 11:58:52.421134 140034245318464 deprecation.py:323] From /home/baogong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1285976 samples, validate on 428659 samples\n",
      "Epoch 1/5\n",
      " 124160/1285976 [=>............................] - ETA: 4:11:43 - loss: 0.0224 - precision: 0.8991 - recall: 0.6619"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1b23f58a692e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          save_weights_only=True)\n\u001b[1;32m      5\u001b[0m \u001b[0matt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattentionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#lstmModel()#attentionModel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0matt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_set_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_set_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint_noatt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "check_point_path_noatt = \"./model/my_att_model.h5\"\n",
    "model_checkpoint_noatt = keras.callbacks.ModelCheckpoint(check_point_path_noatt, verbose=1, save_best_only=True,\n",
    "                                         save_weights_only=True)\n",
    "att_model = attentionModel()#lstmModel()#attentionModel()\n",
    "att_model.fit(x_train_set_all, y_train, \\\n",
    "                validation_data=(x_test_set_all, y_test), \\\n",
    "                epochs=5, batch_size=128, shuffle=True, \\\n",
    "                callbacks=[early_stopping, model_checkpoint_noatt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1689966\n",
       "1      24669\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_smoothing(inputs, epsilon=0.1):\n",
    "#     V = inputs.get_shape().as_list()[-1] # number of channels\n",
    "#     return ((1-epsilon) * inputs) + (epsilon / V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_white_data1(date1, date2):\n",
    "    sql = \"\"\"\n",
    "        select *\n",
    "        from baogong_uuid_white_simulator_url\n",
    "        --tablesample(bucket 1 out of 8 on rand())\n",
    "        where visit_date>='{}'\n",
    "        and visit_date<='{}'\n",
    "        --limit 500000\n",
    "    \"\"\".format(date1, date2)\n",
    "    data = sqlContext.sql(sql).toPandas()\n",
    "    return data\n",
    "###valadation\n",
    "val_black = get_black_data('2019-09-26', '2019-09-30')\n",
    "val_white = get_white_data1('2019-09-26', '2019-09-26')\n",
    "#val_data = get_data('2019-09-26', '2019-09-26')\n",
    "\n",
    "val_white['label'] = 0\n",
    "val_black['label'] = 1\n",
    "val_data = pd.concat([val_white, val_black], axis = 0, ignore_index=True)\n",
    "\n",
    "val_data['len'] = val_data['words'].apply(lambda x: len(x.split('::')))\n",
    "\n",
    "val_data1 = val_data[val_data['len']>=5]\n",
    "val_data1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_deep = val_data1[['words', 'times', 'label']]\n",
    "val_data_deep['times'] = val_data_deep['times'].fillna('0')\n",
    "val_data_deep = val_data_deep.fillna('UNK')\n",
    "\n",
    "val_X_url, val_X_time = get_convert_data(val_data_deep, 200, 1, words_dic)\n",
    "\n",
    "url_val = np.array(val_X_url)\n",
    "time_val = np.array(val_X_time)\n",
    "y_val = val_data_deep['label'].values\n",
    "\n",
    "x_val_set_all = {\"words\": url_val,\n",
    "                   \"times\": time_val,\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = att_model.predict(x_val_set_all, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_deep['pred'] = pred_val[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_deep[val_data_deep['label']==1].shape, val_data_deep[val_data_deep['pred']>=0.99].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_deep[(val_data_deep['label']==1)&(val_data_deep['pred']>=0.99)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14.0/33, 14.0/108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_deep['uuid_did'] = val_data1['uuid_did']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
