{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baogong/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./util/')\n",
    "from utils import load_data_embdding \n",
    "\n",
    "users, movies, ratings = load_data_embdding()\n",
    "\n",
    "data1 = pd.merge(ratings.drop(columns = ['timestamp'],axis = 1), movies, how = 'left', on = 'movieid')\n",
    "data = pd.merge(data1, users, how = 'left', on = 'userid')\n",
    "\n",
    "X = data.drop(columns = ['userid', 'movieid', 'title', 'rating'])\n",
    "Y = data['rating'].values\n",
    "#genres, gender, age, occupationid四个需要embedding的特征，可以分别emb也可以合并以后emb，这里分别做emb\n",
    "set_genres = []\n",
    "for i in movies.index:\n",
    "    set_genres += movies['genres'].iloc[i]\n",
    "set_genres = list(set(set_genres))\n",
    "dic_genres = dict([(j, i) for i,j in enumerate(set_genres)])\n",
    "dic_genres['UNK'] = len(dic_genres)\n",
    "X['genres'] = X['genres'].apply(lambda x: [dic_genres[i] for i in x])\n",
    "x_genres = tf.keras.preprocessing.sequence.pad_sequences(list(X['genres'].values),\n",
    "                                                        value = dic_genres['UNK'],\n",
    "                                                        padding = 'post',\n",
    "                                                        maxlen = 6)\n",
    "\n",
    "\n",
    "dic_gender = {'F':0, 'M':1}\n",
    "X['gender'] = X['gender'].apply(lambda x: [dic_gender[i] for i in x])\n",
    "dic_age = {1:0, 56:1, 25:2, 45:3, 50:4, 35:5, 18:6}\n",
    "X['age'] = X['age'].apply(lambda x: [dic_age[x]])\n",
    "list_occ = list(pd.unique(data['occupationid']))\n",
    "dic_occ = dict([(j, i) for i,j in enumerate(list_occ)])\n",
    "X['occupationid'] = X['occupationid'].apply(lambda x: [dic_occ[x]])\n",
    "\n",
    "x_gender = list(X['gender'].values)\n",
    "x_age = list(X['age'].values)\n",
    "x_occupationid = list(X['occupationid'].values)\n",
    "\n",
    "\n",
    "train_x_genres, test_x_genres, train_y, test_y = train_test_split(np.array(x_genres), Y, random_state=11)\n",
    "train_x_gender, test_x_gender = train_test_split(np.array(x_gender), random_state=11)\n",
    "train_x_age, test_x_age = train_test_split(np.array(x_age), random_state=11)\n",
    "train_x_occupationid, test_x_occupationid = train_test_split(np.array(x_occupationid), random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layerNormalization(Layer):\n",
    "    def __init__(self, l2_rate, epsilon = 1e-8, **kwargs):\n",
    "        self.l2_rate = l2_rate\n",
    "        self.epsilon = epsilon\n",
    "        super(layerNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.w = self.add_weight(name='kernel',\n",
    "                                        shape=(input_dim,),\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        regularizer=l2(self.l2_rate),\n",
    "                                        trainable=True)\n",
    "        self.b = self.add_weight(name='bias',\n",
    "                                     shape=(input_dim,),\n",
    "                                     initializer='Zeros',\n",
    "                                     trainable=True)\n",
    "\n",
    "        super(layerNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        x0 = inputs\n",
    "        mean, variance = tf.nn.moments(inputs, axes=[-1], keepdims=True)\n",
    "        normalized = (inputs - mean) / ( (variance + self.epsilon) ** (.5) )\n",
    "        outputs = self.w * normalized + self.b\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0]\n",
      " [0 1 1]\n",
      " [0 1 2]], shape=(3, 3), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0 0]\n",
      " [1 1]], shape=(2, 2), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[[1 0 3]\n",
      "  [1 5 6]]], shape=(1, 2, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 0 3]\n",
      " [1 5 1]], shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = [[1,2,3],[4,5,6]]\n",
    "b = [[1,0,3],[1,5,1]]\n",
    "condition1 = [[[True,False,False],\n",
    "             [False,True,True]]]\n",
    "condition2 = [[True,False,False],\n",
    "             [False,True,False]]\n",
    "\n",
    "print (tf.where(condition1))#condition1的index\n",
    "print(tf.where(condition2))\n",
    "\n",
    "print (tf.where(condition1, a, b))# a负责True元素， b负责Fasle元素\n",
    "print(tf.where(condition2, a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "import numpy as np;\n",
    " \n",
    "A = list([1,2,3])\n",
    "B = np.array([1,2,3])\n",
    "C = tf.convert_to_tensor(A)\n",
    "D = tf.convert_to_tensor(B)\n",
    "C,D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoding(Layer):\n",
    "    \"\"\"\n",
    "    inputs: 3d tensor. (N, T, E)\n",
    "    maxlen: scalar. Must be >= T\n",
    "    masking: Boolean. If True, padding positions are set to zeros.\n",
    "    returns\n",
    "    3d tensor that has the same shape as inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen, masking=True, **kwargs):\n",
    "        self.maxlen = maxlen\n",
    "        self.masking = masking\n",
    "        super(positionalEncoding, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(positionalEncoding, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        E = inputs.get_shape().as_list()[-1]\n",
    "        N, T = tf.shape(inputs)[0], tf.shape(inputs)[1] # dynamic\n",
    "        # position indices\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # (N, T)\n",
    "\n",
    "        # First part of the PE function: sin and cos argument\n",
    "        position_enc = np.array([\n",
    "            [pos / np.power(10000, (i-i%2)/E) for i in range(E)]\n",
    "            for pos in range(self.maxlen)])\n",
    "\n",
    "        # Second part, apply the cosine to even columns and sin to odds.\n",
    "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "        position_enc = tf.convert_to_tensor(position_enc, tf.float32) # (maxlen, E)\n",
    "\n",
    "        # lookup\n",
    "        outputs = tf.nn.embedding_lookup(position_enc, position_ind)\n",
    "\n",
    "        # masks\n",
    "        if self.masking:\n",
    "            outputs = tf.where(tf.equal(inputs, 0), inputs, outputs)\n",
    "        return tf.dtypes.cast(outputs, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masks = tf.math.equal(x, 0)\n",
    "def mask(inputs, key_masks=None, type=None):\n",
    "    \"\"\"\n",
    "    https://github.com/Kyubyong/transformer/blob/master/modules.py\n",
    "    \"\"\"\n",
    "    padding_num = -2 ** 32 + 1\n",
    "    if type in (\"k\", \"key\", \"keys\"):\n",
    "        key_masks = tf.to_float(key_masks)\n",
    "        key_masks = tf.tile(key_masks, [tf.shape(inputs)[0] // tf.shape(key_masks)[0], 1]) # (h*N, seqlen)\n",
    "        key_masks = tf.expand_dims(key_masks, 1)  # (h*N, 1, seqlen)\n",
    "        outputs = inputs + key_masks * padding_num\n",
    "    elif type in (\"f\", \"future\", \"right\"):\n",
    "        diag_vals = tf.ones_like(inputs[0, :, :])  # (T_q, T_k)\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n",
    "        future_masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])  # (N, T_q, T_k)\n",
    "\n",
    "        paddings = tf.ones_like(future_masks) * padding_num\n",
    "        outputs = tf.where(tf.equal(future_masks, 0), paddings, inputs)#True的都填充了paddings，false填充了inputs\n",
    "    else:\n",
    "        print(\"Check if you entered type correctly!\")\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, masks, key_masks,\n",
    "                                 causality=False, dropout_rate=0.,\n",
    "                                 training=True):\n",
    "    \"\"\"\"\n",
    "    Q: Packed queries. 3d tensor. [N, T_q, d_k].\n",
    "    K: Packed keys. 3d tensor. [N, T_k, d_k].\n",
    "    V: Packed values. 3d tensor. [N, T_k, d_v]\n",
    "    \"\"\"\n",
    "    dk = Q.get_shape().as_list()[-1]\n",
    "    outputs = tf.matmul(Q, tf.transpose(K, (0, 2, 1)))/dk**0.5\n",
    "    if masks:\n",
    "        outputs = mask(outputs, key_masks=key_masks, type=\"key\")\n",
    "    # causality or future blinding masking\n",
    "    if causality:\n",
    "        outputs = mask(outputs, type=\"future\")\n",
    "    \n",
    "    # softmax\n",
    "    outputs = tf.nn.softmax(outputs) \n",
    "    # dropout\n",
    "    outputs = keras.layers.Dropout(rate=dropout_rate)(outputs)\n",
    "\n",
    "    # weighted sum (context vectors)\n",
    "    outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mutiHeadAttention(Layer):\n",
    "    def __init__(self, masks, key_masks, causality, dropout_rate, training, num_heads, size_per_head, **kwargs):\n",
    "        self.masks = masks\n",
    "        self.key_masks = key_masks\n",
    "        self.causality = causality\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = training\n",
    "        self.num_heads = num_heads\n",
    "        self.size_per_head = size_per_head\n",
    " \n",
    "        super(mutiHeadAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_shape = self.num_heads * self.size_per_head\n",
    "        \n",
    "        self.q_dense = keras.layers.Dense(input_shape, use_bias=True)\n",
    "        self.k_dense = keras.layers.Dense(input_shape, use_bias=True)\n",
    "        self.v_dense = keras.layers.Dense(input_shape, use_bias=True)\n",
    "        \n",
    "        super(mutiHeadAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        q, k, v = inputs\n",
    "        \n",
    "        \n",
    "        Q = self.q_dense(q)\n",
    "        K = self.k_dense(k)\n",
    "        V = self.v_dense(v)\n",
    "        \n",
    "        Q_ = tf.concat(tf.split(Q, self.num_heads, axis=2), axis=0) # (num_heads*N, T_q, size_per_head)\n",
    "        K_ = tf.concat(tf.split(K, self.num_heads, axis=2), axis=0) # (num_heads*N, T_k, size_per_head)\n",
    "        V_ = tf.concat(tf.split(V, self.num_heads, axis=2), axis=0) # (num_heads*N, T_k, size_per_head)\n",
    "        \n",
    "        # Attention\n",
    "        outputs = scaled_dot_product_attention(Q_, K_, V_, self.masks, self.key_masks, \n",
    "                                               self.causality, self.dropout_rate, self.training)\n",
    "\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, self.num_heads, axis=0), axis=2 ) # (N, T_q, d_model)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += q\n",
    "        return outputs\n",
    "\n",
    "class mutiHeadAttention(Layer):\n",
    "    def __init__(self, masks, key_masks, causality, dropout_rate, training, num_heads, size_per_head, **kwargs):\n",
    "        self.masks = masks\n",
    "        self.key_masks = key_masks\n",
    "        self.causality = causality\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = training\n",
    "        self.num_heads = num_heads\n",
    "        self.size_per_head = size_per_head\n",
    " \n",
    "        super(mutiHeadAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_shape = self.num_heads * self.size_per_head\n",
    "        \n",
    "        self.q_dense = keras.layers.Dense(input_shape, use_bias=True)\n",
    "        self.k_dense = keras.layers.Dense(input_shape, use_bias=True)\n",
    "        self.v_dense = keras.layers.Dense(input_shape, use_bias=True)\n",
    "        \n",
    "        super(mutiHeadAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        q, k, v = inputs\n",
    "        \n",
    "        \n",
    "        Q = self.q_dense(q)\n",
    "        K = self.k_dense(k)\n",
    "        V = self.v_dense(v)\n",
    "        \n",
    "        Q_ = tf.concat(tf.split(Q, self.num_heads, axis=2), axis=0) # (num_heads*N, T_q, size_per_head)\n",
    "        K_ = tf.concat(tf.split(K, self.num_heads, axis=2), axis=0) # (num_heads*N, T_k, size_per_head)\n",
    "        V_ = tf.concat(tf.split(V, self.num_heads, axis=2), axis=0) # (num_heads*N, T_k, size_per_head)\n",
    "        \n",
    "        # Attention\n",
    "        outputs = scaled_dot_product_attention(Q_, K_, V_, self.masks, self.key_masks, \n",
    "                                               self.causality, self.dropout_rate, self.training)\n",
    "\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, self.num_heads, axis=0), axis=2 ) # (N, T_q, d_model)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += q\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_genres = keras.layers.Input(shape=(6,), name=\"genres\")  \n",
    "embedding_genres = keras.layers.Embedding(output_dim=16, input_dim=len(dic_genres), input_length=6)(input_genres)\n",
    "\n",
    "input_gender = keras.layers.Input(shape=(1,), name=\"gender\")  \n",
    "embedding_gender = keras.layers.Embedding(output_dim=16, input_dim=2, input_length=1)(input_gender)\n",
    "\n",
    "input_age = keras.layers.Input(shape=(1,), name=\"age\")  \n",
    "embedding_age = keras.layers.Embedding(output_dim=16, input_dim=7, input_length=1)(input_age)\n",
    "\n",
    "input_occ = keras.layers.Input(shape=(1,), name=\"occupationid\")  \n",
    "embedding_occ = keras.layers.Embedding(output_dim=16, input_dim=21, input_length=1)(input_occ)\n",
    "\n",
    "embedding_combine = keras.layers.concatenate(inputs=[embedding_genres, embedding_gender, embedding_age,\n",
    "                                                    embedding_occ], axis=1)\n",
    "#embedding_combine = keras.layers.GlobalAveragePooling1D()(embedding_combine) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positionalEncoding对结果影响还是挺大的\n",
    "embedding_combine += positionalEncoding(16, True)(embedding_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'positional_encoding_2/Identity:0' shape=(None, None, 16) dtype=float32>,\n",
       " <tf.Tensor 'concatenate/Identity:0' shape=(None, 9, 16) dtype=float32>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positionalEncoding(16, False)(embedding_combine), embedding_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##att\n",
    "#无0值不需要mask\n",
    "att_layer = mutiHeadAttention(\n",
    "    masks = False, key_masks = None, causality = False, dropout_rate = 0.1, \n",
    "    training = True, num_heads = 4, size_per_head = 4)([embedding_combine, embedding_combine, embedding_combine]) \n",
    "\n",
    "ln = layerNormalization(l2_rate = 0.001, epsilon = 1e-8)(att_layer)\n",
    "ln = keras.layers.Flatten()(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = keras.layers.Dense(1, name = \"outputs\")(ln)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate = 0.001)\n",
    "model = tf.keras.Model(inputs = [input_genres, input_gender, input_age, input_occ], outputs = [outputs])\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['mean_absolute_error', 'mean_squared_error'],\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint_path = \"./model/deepatt.h5\"\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                              save_weights_only=True,\n",
    "                                              save_best_only=True,\n",
    "                                              verbose=1)\n",
    "model.fit(\n",
    "    [train_x_genres, train_x_gender, train_x_age, train_x_occupationid], train_y,\n",
    "    epochs=100, \n",
    "    validation_data=([test_x_genres, test_x_gender, test_x_age, test_x_occupationid], test_y,),\n",
    "    batch_size=256, shuffle=True,\n",
    "    callbacks=[early_stopping, cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###valadation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
